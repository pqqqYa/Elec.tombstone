# 一、主要原理
## 1. 预测函数
随机确定一次函数`y=ωx`后面通过代价函数调整ω来拟合
## 2. 代价函数
均方误差
$$e_n=(y_n-\omega*x_n)^2$$
求所有误差的平均值
$$e=\frac{1}{n}(a*\omega^2+b*\omega+c)$$
![[预测函数和代价函数图示.png]]
## 3. 梯度计算
寻找代价函数中ω的最低点，这个寻找最低点的过程就是梯度下降
梯度Gradient就是代价函数的导数，抛物线的曲线斜率
## 4. 学习率（Learning Rate）
也叫做步长，一点一点的让ω接近最低点
## 5. 循环迭代
$$\omega_新=\omega_旧-斜率*学习率$$
# 二、步骤
## 1. 定义代价函数
## 2. 选择起始点
## 3. 计算梯度
## 4.按学习率前进
## 5. 重复3、4步直到找到最低点

# 三、实际情况
函数不一定只是一次的或者二维的等等
代价函数可能有全局最优或者局部最优（最小值和极小值）
# 四、分类
1. 批量梯度下降BGD（高精度，速度慢）
2. 随机梯度下降法SGD（提高速度，降低精准度）
3. 小批量梯度下降法（简洁高效）

AdaGrad动态学习率
RMSProp优化动态学习率
AdaDelta无需设置学习率
Adam融合AdaGrad和RMSProp
Momentum模拟动量
FTRL........

杂谈：
在机器学习中，代价函数和损失函数是等价的概念，通常用来衡量模型在训练集上的预测结果与实际值之间的差距。在深度学习中，损失函数通常是指针对训练样本计算出来的，能够度量模型对于单个样本的拟合程度，然后通过所有样本的损失函数的平均值来衡量模型整体的表现。而代价函数的作用是对整个训练集中所有样本的损失函数求和或者求平均得到。

简单来说，损失函数和代价函数的区别在于计算的对象不同，损失函数计算单个样本的误差，代价函数计算整个训练集的误差。在模型训练过程中，我们通过不断迭代，优化代价函数的值，使其达到最小值，以达到对输入数据的正确拟合，从而得到更加准确、实用的预测函数。