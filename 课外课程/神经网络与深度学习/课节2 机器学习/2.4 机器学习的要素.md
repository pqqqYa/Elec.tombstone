机器学习的4个要素：数据，模型，学习准则，优化算法
# 模型
以回归(Regression)为例
![[回归-线性模型.png]]
$$f(x;θ)=w^\intercal x+b$$
线性模型
![[回归-非线性模型.png]]
$$f(x;θ)=w^\intercal \phi(x)+b$$
非线性模型
如果Φ(x)为可学习的非线性基函数,f(x,0)就等价于神经网络。
# 学习准则
一个好的模型应该在所有取值上都与真实映射函数一致
$$|f(x,\theta ^*)-y|< \epsilon ,\qquad \forall(x,y) \in  x\times y$$
## 损失函数(Loss Function )
损失函数是一个非负实数函数，用来量化模型预测和真实标签之间的差异
### 平方损失函数( Quadratic Loss Function)
$$\mathfrak{L} (y,f(x;\theta))=\frac{1}{2}(y-f(x;\theta))^2$$
## 期望风险(Expected Risk)
$$\mathcal{R}(\theta)=\mathbb{E}_{(x,y)\sim p_r(x,y)}[\mathfrak{L}(y,f(x,\theta))]$$
`pr(x,y)`是真实的数据分布
`L(y,f(x,θ))`是损失函数
期望风险可以近似为
$$\left \{ (x^{(n)},y^{(n)}) \right \}^{N}_{n=1} $$
训练数据
$$\mathcal{R}^{emp}_{\mathcal{D}}(\theta)=\frac{1}{N}\sum_{n=1}^{N}\mathfrak{L}(y^{(n)},f(x^{(n)};\theta ))  $$
经验风险(Empirical Risk)：每个样本上计算损失求平均
经验风险最小化(Empirical Risk Minimization,ERM)寻找一个参数`θ*`，使得经验风险函数最小化。
# 最优化问题
机器学习问题转化成为一个最优化问题
$$\sideset{}{}{min}_x^ {}f(x)$$
![[最优化问题.png]]
# 梯度下降法
[[梯度下降法]]
## 学习率
学习率是十分重要的超参数
![[学习率好坏比较.png]]
## 随机梯度下降法
在每次迭代时只采集一个样本
$$\theta_{t+1}=\theta_t-\alpha\frac{{\partial \mathcal{L}(y^{(n)},f(x^{(n)};\theta)}}{\partial \theta}$$
当经过足够次数的迭代时，随机梯度下降也可以收敛到局部最优解
* 优点：每次计算开销小，支持在线学习
* 缺点:无法充分利用计算机的并行计算能力，每次都是一个样本
## 小批量（Mini-Batcb）随机梯度下降法
随机选取一小部分训练样本来计算梯度并更新参数
既可以兼顾随机梯度下降法的优点，也可以提高训练效率

